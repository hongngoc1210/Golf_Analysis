# ğŸŒï¸ Golf Swing Quality Assessment with Neural Additive Models (NAM)

## ğŸ“Œ Overview

This project proposes an **end-to-end explainable machine learning pipeline** for **golf swing quality assessment**, combining:

* **Stage 1**: Feature selection using tree-based models (LightGBM + SHAP)
* **Stage 2**: Explainable **Neural Additive Model (NAM)** for **binary classification**
* **XAI**: Feature-level contribution analysis
* **LLM-based feedback generation** (Gemini / Gemma / Template fallback)

The system not only predicts whether a golf swing is **GOOD** or **BAD**, but also explains *why* and provides **human-readable coaching feedback**.

---

## ğŸ¯ Problem Definition

* **Input**: Motion-derived golf swing features (angles, ratios, positions)
* **Output**:

  * Binary classification:

    * `0` â†’ Bad swing
    * `1` â†’ Good swing
  * Feature contributions
  * Personalized coaching feedback

---

## ğŸ§  Core Ideas

1. **Interpretability-first modeling**
   Each feature contributes independently via a small neural network:
   [
   \text{logit} = b + \sum_i f_i(x_i)
   ]

2. **Two-stage learning**

   * Stage 1: Learn global feature importance
   * Stage 2: Learn interpretable per-feature effects

3. **Human-in-the-loop explainability**

   * Model â†’ Explainer â†’ Reasoner â†’ LLM â†’ Feedback

---

## ğŸ—‚ï¸ Project Structure

```text
DataStorm/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train_stage2.csv
â”‚       â”œâ”€â”€ val_stage2.csv
â”‚       â””â”€â”€ test_stage2.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ nam.py                 # NAMClassifier + Loss
â”‚   â”‚   â””â”€â”€ trainer.py             # Training loop
â”‚   â”‚
â”‚   â”œâ”€â”€ xai/
â”‚   â”‚   â””â”€â”€ explainer.py           # NAMExplainerClassification
â”‚   â”‚
â”‚   â”œâ”€â”€ reasoning/
â”‚   â”‚   â””â”€â”€ technical_reasoner_classification.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ llm_consumer.py        # Gemini / Gemma / Template
â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ load_config.py
â”‚       â”œâ”€â”€ metrics.py
â”‚       â””â”€â”€ nam_export.py
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ nam_classifier_stage2/
â”‚   â”‚       â”œâ”€â”€ best_model.pth
â”‚   â”‚       â””â”€â”€ feature_list.json
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â””â”€â”€ nam_predictions.json
â”‚   â””â”€â”€ reports/
â”‚       â””â”€â”€ nam_test_metrics.json
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ nam_classifier.yaml
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ inference.py
â”œâ”€â”€ evaluate_test.py
â”œâ”€â”€ generate_feedback.py
â””â”€â”€ README.md
```

---

## âš™ï¸ Pipeline Description

### ğŸ”¹ Stage 1 â€“ Feature Selection (LightGBM)

* Train a tree-based model on all available features
* Use SHAP values to estimate global feature importance
* Select **Top-N features**
* Create `*_stage2.csv` datasets

> Purpose: Reduce noise & stabilize NAM training

---

### ğŸ”¹ Stage 2 â€“ NAM Binary Classification

* Model: **Neural Additive Model**
* Each feature has its own sub-network
* Output:

  * Logit
  * Probability
  * Per-feature contribution

**Loss function**:
[
\mathcal{L} =
\text{BCEWithLogits}

* \lambda_1 \lVert \theta \rVert^2
* \lambda_2 \mathbb{E}[f_i(x_i)^2]
  ]

---

### ğŸ”¹ Evaluation Metrics

* Accuracy
* F1-score
* ROC-AUC
* Confusion Matrix

Evaluation is performed strictly on **held-out test set**.

---

### ğŸ”¹ Explainability (XAI)

`NAMExplainerClassification` produces:

* Prediction (`GOOD` / `BAD`)
* Probability
* Top positive features
* Top negative features
* Full contribution list

All outputs are **JSON-safe**.

---

### ğŸ”¹ Technical Reasoning Layer

`TechnicalReasonerClassification` converts raw contributions into:

* Key technical issues
* Severity estimation
* Strengths vs weaknesses
* Structured reasoning schema for LLM

This layer ensures:

* No hallucination
* Domain grounding
* Stable feedback

---

### ğŸ”¹ LLM Feedback Generation

Supported backends:

* Gemini API
* Gemma (via API)
* Template fallback

LLM receives **structured reasoning**, not raw numbers.

Output:

* Overall assessment
* Technical explanation
* Improvement guidance
* Drills
* Encouragement (Vietnamese)

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Train Stage 2 NAM

```bash
python train.py --config configs/nam_classifier.yaml
```

---

### 2ï¸âƒ£ Evaluate on Test Set

```bash
python evaluate_test.py \
  --config configs/nam_classifier.yaml \
  --model_dir outputs/models/nam_classifier_stage2 \
  --test_data datasets/processed/test_stage2.csv
```

---

### 3ï¸âƒ£ Run Inference

```bash
python inference.py \
  --config configs/nam_classifier.yaml \
  --data datasets/processed/test_stage2.csv \
  --model outputs/models/nam_classifier_stage2/best_model.pth \
  --output outputs/inference/nam_predictions.json
```

---

### 4ï¸âƒ£ Generate Technical Reasoning

```bash
python technical_reasoner_classification.py \
  --input outputs/inference/nam_predictions.json \
  --output outputs/inference/technical_reasoning.json
```

---

### 5ï¸âƒ£ Generate LLM Feedback

```bash
python generate_feedback.py \
  --input outputs/inference/technical_reasoning.json
```

---

## ğŸ“Š Key Advantages

* âœ… Fully explainable architecture
* âœ… Feature-level interpretability
* âœ… Stable reasoning before LLM
* âœ… Suitable for academic research
* âœ… Ready for real coaching systems

---

## ğŸ“š Intended Use

* Master / Bachelor thesis
* Sports analytics research
* Explainable AI case study
* Intelligent coaching systems

---

## ğŸ“Œ Notes

* The project is designed to be **model-agnostic at Stage 1**
* NAM architecture is extensible to:

  * Regression
  * Multi-class classification
* LLM backend can be swapped without retraining

---

## âœï¸ Author

Developed as part of an academic research project on **Explainable AI for Sports Performance Analysis**.
